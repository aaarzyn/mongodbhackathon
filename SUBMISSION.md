# Team Name
ContextScope

# Project Description
- ContextScope Eval is an open-source evaluation and observability framework for agent-to-agent context sharing. It measures how effectively autonomous agents pass and use information, tracking context fidelity, relevance drift, temporal coherence, and utility across multi-agent pipelines. Unlike traditional LLM benchmarks or AI Eval tool (which focus on accuracy or helpfulness), ContextScope measures information flow quality with high resolution visualization that provides actionable insights such like how context survives, transforms, and degrades as it moves through agents.

- The problem it solves: As multi-agent systems grow more complex, evaluating how well agents communicate and share memory becomes essential. Looking at the single points of evaluation metrics is no longer effective, current evaluation tools (eg. LangSmith, Braintrust, Traceloop) focus on reasoning steps or final outputs but not on context transmission itself. ContextScope Eval fills this gap.



# Public GitHub Repository
github.com/aaarzyn/mongodbhackathon

# Demo Video


# Please indicate which MongoDB product / features were used
MongoDB Atlas, used clusters, added to existing data
Voyager AI for embeddings and retrieval work

# Partner Technologies Used
Modelence for original set up (ended up transitioning away from the UI)
Fireworks (for responses)
Warp (for coding)
Meta: Llama model (for evaluation of handoffs)